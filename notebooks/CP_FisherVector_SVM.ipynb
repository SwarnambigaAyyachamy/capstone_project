{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming workflow\n",
    "### I - Build image dictionary\n",
    "### II - Fit GMM model\n",
    "### III - Compute Fisher Vector for Training and Test set\n",
    "### IV - Grid Search Cross-validation SVM\n",
    "### V - Prediction on test set\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FISHER VECTOR PARAMETERS\n",
    "# =============================================================================\n",
    "n_cmp = 20 # pca components\n",
    "k = 256 # gmm centroids\n",
    "fnum = 4096 # number of descriptors\n",
    "\n",
    "#==============================================================================\n",
    "# IMAGE PARAMETERS\n",
    "#==============================================================================\n",
    "path = '/Users/goncalofigueira/Documents/capstone_project/datasets/ICIAR2018_BACH_Challenge/Photos/'\n",
    "im_type = '.tif'\n",
    "test_perc = 0.2\n",
    "\n",
    "#==============================================================================\n",
    "# GET IMAGE LIST AND INFO\n",
    "#==============================================================================\n",
    "im_folder = np.array(getFileList(path,im_type)) # image list\n",
    "# Load csv with image information\n",
    "im_info = pd.read_csv(getFileList(path,'.csv')[0], header = None)\n",
    "im_info.columns = ['filename','target']\n",
    "\n",
    "# =============================================================================\n",
    "# MATCH IMAGE LIST AND LABELS\n",
    "# =============================================================================\n",
    "im_info = sortTarget(im_folder,im_info)\n",
    "le = preprocessing.LabelEncoder()\n",
    "T = im_info.target\n",
    "T = np.array(le.fit_transform(T)) # array with targets (0-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset in train (80%) and test set (20%)\n",
    "Here I used stratified shuffle to get equal number of sampes per class in the train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = test_perc, random_state = 0)\n",
    "for train_index, test_index in split.split(im_folder,T):\n",
    "    train_files = train_index\n",
    "    test_files = test_index\n",
    "\n",
    "y_train = T[train_files]\n",
    "y_test = T[test_files]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I - Build SIFT dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For each image in training set:\n",
    "#### 1. Image pre - processing\n",
    "#### 2. Compute SIFT descrpitor\n",
    "#### 3. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dictionary = []\n",
    "\n",
    "for file  in tqdm(im_folder[train_files]):\n",
    "    \n",
    "    # 1. Read image with opencv [1536, 2048, 3]\n",
    "    im = ReadImage(file)\n",
    "    \n",
    "    #3.  to gray\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # 5. Compute SIFT descriptors (fnum = 4096)\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = fnum)\n",
    "    kp, descriptors = sift.detectAndCompute(im_gray, None)\n",
    "    \n",
    "    # 6. Normalize descriptor vector\n",
    "    descriptors /= (descriptors.sum(axis=1, keepdims=True) + 1e-7)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    "\n",
    "    # 7. Append to array\n",
    "    dictionary.append(descriptors)\n",
    "\n",
    "# list to np array: []\n",
    "dictionary = np.asarray(dictionary)\n",
    "dictionary = np.concatenate(dictionary).astype(None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II - Fit GMM model\n",
    "#### Standardise descriptor features\n",
    "#### Apply PCA-withening\n",
    "#### fit gmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPLY PCA TO DESCRIPTORS LIBRARY\n",
    "# =============================================================================\n",
    "\n",
    "# scalling\n",
    "sift_scaler = preprocessing.StandardScaler()\n",
    "descriptors = sift_scaler.fit_transform(descriptors)\n",
    "\n",
    "# apply pca (n_cmp = 20)\n",
    "sift_pca = PCA(n_components=n_cmp,whiten=True)\n",
    "dictionary = sift_pca.fit_transform(dictionary)\n",
    "\n",
    "## fit GMM (k = 256)\n",
    "gmm_pca = GaussianMixture(n_components = k, covariance_type = \"diag\").fit(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## III - Compute Fisher Vector for Training and Test set\n",
    "#### 1. Apply same pre-processing\n",
    "#### 2. Apply scalling and pca transformation\n",
    "#### 3. Compute Fisher Vector\n",
    "#### 4. Normalise Fisher Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    " # COMPUTE FISHER VECTORS FOR TRAIN SET\n",
    "# =============================================================================\n",
    "    \n",
    "X_train = np.empty((y_train.shape[0],k+2*n_cmp*k))\n",
    "idx = 0\n",
    "for file in tqdm(im_folder[train_files]):\n",
    "    X_train[idx,:] = FeatureExtract(file, nkeys = fnum, pca = sift_pca, gmm = gmm_pca, scaler = sift_scaler)\n",
    "    idx += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    " # COMPUTE FISHER VECTORS FOR TEST SET\n",
    "# =============================================================================\n",
    "    \n",
    "X_test = np.empty((y_test.shape[0],k+2*n_cmp*k))\n",
    "idx = 0\n",
    "for file in tqdm(im_folder[test_files]):\n",
    "    X_test[idx,:] = FeatureExtract(file, nkeys = fnum, pca = sift_pca, gmm = gmm_pca, scaler = sift_scaler)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FeatureExtract function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FeatureExtract(im_file, nkeys, pca, gmm, scaler):\n",
    "    # read image\n",
    "    im = ReadImage(im_file)\n",
    "    \n",
    "    # to gray\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # extract SIFT descriptors\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = nkeys)\n",
    "    kp, descriptors = sift.detectAndCompute(im_gray, None)\n",
    "    descriptors /= (descriptors.sum(axis=1, keepdims=True) + 1e-7)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    "    \n",
    "    # apply pca transform\n",
    "    descriptors = scaler.transform(descriptors)\n",
    "    descriptors = pca.transform(descriptors)\n",
    "    \n",
    "    # compute Fisher Vector\n",
    "    fv = computeFV(descriptors, gmm)\n",
    "    \n",
    "    \n",
    "    # power-normalization\n",
    "    fv = np.sign(fv) * np.abs(fv) ** 0.5\n",
    "    # L2 normalize\n",
    "    fv /= np.sqrt(np.sum(fv ** 2))\n",
    "    \n",
    "    return fv\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### computeFV function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def computeFV(xx, gmm):\n",
    "    \"\"\"Computes the Fisher vector on a set of descriptors.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    xx: array_like, shape (N, D) or (D, )\n",
    "        The set of descriptors\n",
    "\n",
    "    gmm: instance of sklearn mixture.GMM object\n",
    "        Gauassian mixture model of the descriptors.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    fv: array_like, shape (K + 2 * D * K, )\n",
    "        Fisher vector (derivatives with respect to the mixing weights, means\n",
    "        and variances) of the given descriptors.\n",
    "\n",
    "    Reference\n",
    "    ---------\n",
    "    J. Krapac, J. Verbeek, F. Jurie.  Modeling Spatial Layout with Fisher\n",
    "    Vectors for Image Categorization.  In ICCV, 2011.\n",
    "    http://hal.inria.fr/docs/00/61/94/03/PDF/final.r1.pdf\n",
    "\n",
    "    \"\"\"\n",
    "    xx = np.atleast_2d(xx)\n",
    "    N = xx.shape[0]\n",
    "    # Compute posterior probabilities.\n",
    "    Q = gmm.predict_proba(xx)  # NxK\n",
    "   # print(Q.shape)\n",
    "    # Compute the sufficient statistics of descriptors.\n",
    "    Q_sum = np.sum(Q, 0)[:, np.newaxis] / N\n",
    "    Q_xx = np.dot(Q.T, xx) / N\n",
    "    Q_xx_2 = np.dot(Q.T, xx ** 2) / N\n",
    "\n",
    "    # Compute derivatives with respect to mixing weights, means and variances.\n",
    "    d_pi = Q_sum.squeeze() - gmm.weights_\n",
    "    d_mu = Q_xx - Q_sum * gmm.means_\n",
    "    d_sigma = (\n",
    "            - Q_xx_2\n",
    "            - Q_sum * gmm.means_ ** 2\n",
    "            + Q_sum * gmm.covariances_\n",
    "            + 2 * Q_xx * gmm.means_)\n",
    "\n",
    "    # Merge derivatives into a vector.\n",
    "    return np.hstack((d_pi, d_mu.flatten(), d_sigma.flatten()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IV - Grid Search Cross-validation SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "C_range = np.logspace(-2, 10, 13)\n",
    "g_range = np.logspace(-9, 3, 13)\n",
    "cv = StratifiedShuffleSplit(n_splits=5, test_size=test_perc, random_state=42)\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': g_range, 'C': C_range}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = GridSearchCV(SVC(cache_size=2500), tuned_parameters, cv=cv,\n",
    "                   scoring='%f1_macro' % score, n_jobs=-1)\n",
    "\n",
    "# =============================================================================\n",
    "#     COMPUTE PARAMETERS\n",
    "# =============================================================================\n",
    "t2 = time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "elapsed2 = time.time() - t2\n",
    "print()\n",
    "print(clf.best_params_)\n",
    "print()\n",
    "print('Training time: ', elapsed2)\n",
    "\n",
    "# ==============================================================================\n",
    "# TRAIN SET\n",
    "# ==============================================================================\n",
    "clf2 = clf.best_estimator_\n",
    "print(\"Classification on training set:\")\n",
    "y_true, y_pred = y_train, clf2.predict(X_train)\n",
    "#print('Confusion matrix:')\n",
    "#print(confusion_matrix(y_true, y_pred))\n",
    "print(\" Train set f1 score: \" + str(f1_score(y_true, y_pred, average='macro')))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## V - Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "# TESTING\n",
    "# ==============================================================================\n",
    "y_true, y_pred = y_test, clf2.predict(X_test)\n",
    "print(\"Classification on test set:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
