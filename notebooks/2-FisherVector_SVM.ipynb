{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/goncalofigueira/Desktop/gf/capstone_project/src\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python2\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Fri Jan 12 17:36:09 2018\n",
    "\n",
    "@author: goncalofigueira\n",
    "\"\"\"\n",
    "# =============================================================================\n",
    "# MODULES IMPORT\n",
    "# =============================================================================\n",
    "from __future__ import print_function\n",
    "import sys\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import f1_score\n",
    "import scipy as sc\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "from sklearn import preprocessing\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "os.chdir(\"/Users/goncalofigueira/Desktop/gf/capstone_project/src/\")\n",
    "print(os.getcwd())\n",
    "\n",
    "sys.path.append(os.path.join(os.getcwd(), \"utils\"))\n",
    "from data_utils import getFileList, sortTarget, ReadImage, rgb_normalized\n",
    "from FisherVector import FeatureExtract, computeFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#==============================================================================\n",
    "# IMAGE PARAMETERS\n",
    "#==============================================================================\n",
    "path = '/Users/goncalofigueira/Documents/capstone_project/datasets/ICIAR2018_BACH_Challenge/Photos/'\n",
    "im_type = '.tif'\n",
    "test_perc = 0.2 # test set percentage\n",
    "\n",
    "#==============================================================================\n",
    "# GET IMAGE LIST AND INFO\n",
    "#==============================================================================\n",
    "im_folder = np.array(getFileList(path,im_type)) # image list\n",
    "# Load csv with image information\n",
    "im_info = pd.read_csv(getFileList(path,'.csv')[0], header = None)\n",
    "im_info.columns = ['filename','target']\n",
    "\n",
    "# =============================================================================\n",
    "# MATCH IMAGE LIST AND LABELS\n",
    "# =============================================================================\n",
    "im_info = sortTarget(im_folder,im_info)\n",
    "le = preprocessing.LabelEncoder()\n",
    "T = im_info.target\n",
    "T = np.array(le.fit_transform(T))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size 320 images\n",
      "Test set size 80 images\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# TRAIN/TEST SPLIT\n",
    "# =============================================================================\n",
    "split = StratifiedShuffleSplit(n_splits = 1, test_size = test_perc, random_state = 0)\n",
    "for train_index, test_index in split.split(im_folder,T):\n",
    "    train_files = train_index\n",
    "    test_files = test_index\n",
    "\n",
    "y_train = T[train_files]\n",
    "y_test = T[test_files]\n",
    "\n",
    "print('Train set size', y_train.shape[0], 'images')\n",
    "print('Test set size', y_test.shape[0], 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# FISHER VECTOR PARAMETERS\n",
    "# =============================================================================\n",
    "n_cmp = 10 # pca components\n",
    "k = 512 # gmm n centroids\n",
    "fnum = 8192 # n sift descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# EXTRACT SIFT DESCRIPTORS FROM TRAIN SET\n",
    "# =============================================================================\n",
    "dictionary = []\n",
    "for file  in tqdm(im_folder[train_files]):\n",
    "    im = ReadImage(file)\n",
    "    im_gray = cv2.cvtColor(im, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Extract sift descriptors\n",
    "    sift = cv2.xfeatures2d.SIFT_create(nfeatures = fnum)\n",
    "    kp, descriptors = sift.detectAndCompute(im_gray, None)\n",
    "    \n",
    "    descriptors /= (descriptors.sum(axis=1, keepdims=True) + 1e-7)\n",
    "    descriptors = np.sqrt(descriptors)\n",
    " \n",
    "    dictionary.append(descriptors)\n",
    "\n",
    "    \n",
    "dictionary = np.asarray(dictionary)\n",
    "dictionary = np.concatenate(dictionary).astype(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# APPLY PCA TO DESCRIPTORS LIBRARY\n",
    "# =============================================================================\n",
    "sift_scaler = preprocessing.StandardScaler()\n",
    "descriptors = sift_scaler.fit_transform(descriptors)\n",
    "\n",
    "sift_pca = PCA(n_components=n_cmp,whiten=True)\n",
    "dictionary = sift_pca.fit_transform(dictionary)\n",
    "dictionary = np.float32(dictionary)\n",
    "\n",
    "#with open('pca_transform.pickle', 'wb') as handle:\n",
    "#    pickle.dump(sift_pca, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "#with open('scaler.pickle', 'wb') as handle:\n",
    "#    pickle.dump(sift_scaler, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## =============================================================================\n",
    "## BUILD DICTIONARY MODEL\n",
    "## =============================================================================\n",
    "gmm_pca = GaussianMixture(n_components = k, covariance_type = \"diag\").fit(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#with open('gmm4.pickle', 'rb') as handle:\n",
    "#    gmm_pca = pickle.load(handle)    \n",
    "#with open('pca4.pickle', 'rb') as handle:\n",
    "#    sift_pca = pickle.load(handle)   \n",
    "#with open('scaler4.pickle', 'rb') as handle:\n",
    "#    sift_scaler = pickle.load(handle)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    " # COMPUTE FISHER VECTORS FOR TRAIN SET\n",
    "# =============================================================================\n",
    "X_train = np.empty((y_train.shape[0],k+2*dictionary.shape[1]*k))\n",
    "    \n",
    "idx = 0\n",
    "for file in tqdm(im_folder[train_files]):\n",
    "    X_train[idx,:] = FeatureExtract(file, nkeys = fnum, pca = sift_pca, gmm = gmm_pca, scaler = sift_scaler)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    " # COMPUTE FISHER VECTORS FOR TEST SET\n",
    "# =============================================================================  \n",
    "X_test = np.empty((y_test.shape[0],k+2*dictionary.shape[1]*k))\n",
    "\n",
    "idx = 0\n",
    "for file in tqdm(im_folder[test_files]):\n",
    "    X_test[idx,:] = FeatureExtract(file, nkeys = fnum, pca = sift_pca, gmm = gmm_pca, scaler = sift_scaler)\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('X_train4.pickle', 'rb') as handle:\n",
    "#    X_train = pickle.load(handle)  \n",
    "#with open('X_test4.pickle', 'rb') as handle:\n",
    "#    X_test = pickle.load(handle)  \n",
    "#print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PRE PROCESSING   \n",
    "# =============================================================================\n",
    "#ch2 = SelectKBest(chi2, k=100)\n",
    "#X_train = ch2.fit_transform(X_train, y_train)\n",
    "#X_test = ch2.transform(X_test)\n",
    "#\n",
    "\n",
    "##PCA\n",
    "pca = PCA(n_components = 20, whiten=True,random_state=42)\n",
    "X_train = pca.fit_transform(X_train)\n",
    "X_test = pca.transform(X_test)  \n",
    "#\n",
    "## SCALING data\n",
    "#scaler = preprocessing.MinMaxScaler(feature_range=(-1, 1), copy=True)\n",
    "scaler = preprocessing.StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# SVM MODEL parameters\n",
    "# =============================================================================\n",
    "sss = RepeatedStratifiedKFold(n_splits=5, n_repeats=200, random_state=42)\n",
    "\n",
    "C_range = 2. ** np.arange(0, 1, step=0.05) # finer search\n",
    "g_range = np.logspace(-2, -1, 20)\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': g_range, 'C': C_range}]\n",
    "#tuned_parameters = [{'kernel': ['linear'],  'C': C_range}]\n",
    "#tuned_parameters = [{'kernel': ['poly'],  'C': C_range,'degree': [2,3,4,5,6,7,8]}]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for f1\n",
      "\n",
      "\n",
      "Best parameters set found on development set:\n",
      "{'kernel': 'rbf', 'C': 1.9318726578496912, 'gamma': 0.088586679041008226}\n",
      "\n",
      "Training time:  1420.43013191\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    " # GRID SEARCH\n",
    "# ==============================================================================\n",
    "scores = ['f1']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(cache_size=2000, random_state = 42, decision_function_shape='ovr'), tuned_parameters, cv=sss,\n",
    "                       scoring='%s_macro' % score, n_jobs=-1)\n",
    "\n",
    "# =============================================================================\n",
    "#     COMPUTE PARAMETERS\n",
    "# =============================================================================\n",
    "    t2 = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    elapsed2 = time.time() - t2\n",
    "    print()\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "\n",
    "    print('Training time: ', elapsed2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on training set:\n",
      "Confusion matrix:\n",
      "[[80  0  0  0]\n",
      " [ 0 80  0  0]\n",
      " [ 0  0 80  0]\n",
      " [ 0  0  0 80]]\n",
      " Train set f1 score: 1.0\n"
     ]
    }
   ],
   "source": [
    "# EVALUATION\n",
    "# ==============================================================================\n",
    "# TRAIN SET\n",
    "# ===========================================================================\n",
    "clf2 = clf.best_estimator_\n",
    "#s print(clf2)\n",
    "print(\"Classification on training set:\")\n",
    "y_true, y_pred = y_train, clf2.predict(X_train)\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(\" Train set f1 score: \" + str(f1_score(y_true, y_pred, average='macro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification on test set:\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.74      0.85      0.79        20\n",
      "          1       0.79      0.75      0.77        20\n",
      "          2       0.88      0.75      0.81        20\n",
      "          3       0.81      0.85      0.83        20\n",
      "\n",
      "avg / total       0.81      0.80      0.80        80\n",
      "\n",
      "Confusion matrix:\n",
      "[[17  2  1  0]\n",
      " [ 1 15  1  3]\n",
      " [ 2  2 15  1]\n",
      " [ 3  0  0 17]]\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# TESTING\n",
    "# ==============================================================================\n",
    "y_true, y_pred = y_test, clf2.predict(X_test)\n",
    "y_pred_ci = clf.decision_function(X_test)\n",
    "print(\"Classification on test set:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('Confusion matrix:')\n",
    "print(confusion_matrix(y_true, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
